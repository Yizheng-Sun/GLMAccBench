# Default Training Configuration

# Output settings
output_dir: "./results"
logging_dir: "./logs"
model_path: "./models/nucleotide-transformer-500m-human-ref"
dataset_path: "./datasets/nucleotide_transformer_downstream_tasks_revised"
task: "promoter_all"
max_length: 512

# Training hyperparameters
num_train_epochs: 3
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 1

# Optimization
learning_rate: 5e-5
weight_decay: 0.01
warmup_steps: 500
lr_scheduler_type: "linear"

# Evaluation and checkpointing
evaluation_strategy: "steps"
eval_steps: 500
save_strategy: "steps"
save_steps: 1000
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: True

# Mixed precision
fp16: True
bf16: false
gradient_checkpointing: True

# Model-specific
freeze_backbone: True
reinitialize_classifier: True
use_gradient_clipping_callback: false
gradient_clip_threshold: 10.0

# Other
seed: 42
push_to_hub: false
report_to: ["tensorboard"]
